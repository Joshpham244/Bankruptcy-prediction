{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Preprocess_3042022.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HZWsWCYpe5BG"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd /content/drive/My Drive/newdata"],"metadata":{"id":"ja8sRxQ_fA9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install fancyimpute\n","!pip install impyute"],"metadata":{"id":"6RVHbVKpLpf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To supress warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","# Basic Libraries for Data organization, Statistical operations and Plotting\n","import numpy as np\n","import pandas as pd\n","%matplotlib inline\n","# For loading .arff files\n","from scipy.io import arff\n","# To analyze the type of missing data\n","import missingno as msno\n","# Library for performing k-NN and MICE imputations \n","import fancyimpute\n","# Library to perform Expectation-Maximization (EM) imputation\n","import impyute as impy\n","# To perform mean imputation\n","#To perform kFold Cross Validation\n","from sklearn.model_selection import KFold\n","# Formatted counter of class labels\n","from collections import Counter\n","# Ordered Dictionary\n","from collections import OrderedDict\n","# Library imbalanced-learn to deal with the data imbalance. To use SMOTE oversampling\n","from imblearn.over_sampling import SMOTE \n","from scipy.stats.mstats import winsorize\n","\n","\n","# Impoting classification models\n","from xgboost import XGBClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from imblearn.ensemble import BalancedBaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","import random\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import precision_recall_curve"],"metadata":{"id":"u6r82jvjfJ8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv('/content/drive/MyDrive/newdata/train_bankcruptcy.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/newdata/test_bankcruptcy.csv')"],"metadata":{"id":"ljHHVt39fM-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.head()"],"metadata":{"id":"F6jzdbnxfrDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Module"],"metadata":{"id":"7K8nH893MIHa"}},{"cell_type":"code","source":["def load_dataframes(a):\n","    return [pd.DataFrame(data_i_year[0]) for data_i_year in a]\n","def set_new_headers(dataframes):\n","    cols = ['X' + str(i+1) for i in range(len(dataframes[0].columns)-1)]\n","    cols.append('Y')\n","    for df in dataframes:\n","        df.columns = cols\n","\n"],"metadata":{"id":"lVzfpG7LMIlp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_test_headers(dataframes):\n","    cols = ['X' + str(i+1) for i in range(len(dataframes[0].columns))]\n","    for df in dataframes:\n","        df.columns = cols\n"],"metadata":{"id":"mQWdEJZmeTBI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocess"],"metadata":{"id":"gBE-gcRr54PN"}},{"cell_type":"code","source":["df_train = df_train.replace({'?':np.nan})\n","df_test = df_test.replace({'?':np.nan})\n","df_train = df_train.rename(columns={\"class\":\"TARGET\"})"],"metadata":{"id":"0mmBZ0uggkHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test.shape"],"metadata":{"id":"eINyKEJrd4kB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index = 0 \n","while(index<=66):\n","  colname = df_train.columns[index]\n","  col = getattr(df_train, colname)\n","  df_train[colname] = col.astype(float)\n","  index+=1"],"metadata":{"id":"CKig-uRt7GgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index = 0 \n","while(index<=65):\n","  colname = df_test.columns[index]\n","  col = getattr(df_test, colname)\n","  df_test[colname] = col.astype(float)\n","  index+=1"],"metadata":{"id":"l2YP0TVqNOEY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_1 = df_train[df_train['forecasting period']==1]\n","df_train_2 = df_train[df_train['forecasting period']==2]\n","df_train_3 = df_train[df_train['forecasting period']==3]\n","df_train_4 = df_train[df_train['forecasting period']==4]\n","dataframes = [df_train_1, df_train_2, df_train_3, df_train_4]\n","set_new_headers(dataframes)    \n"],"metadata":{"id":"MCednIdrOoe3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test_1 = df_test[df_test['forecasting period']==1]\n","df_test_2 = df_test[df_test['forecasting period']==2]\n","df_test_3 = df_test[df_test['forecasting period']==3]\n","df_test_4 = df_test[df_test['forecasting period']==4]\n","dataframes_test = [df_test_1, df_test_2, df_test_3, df_test_4]\n","set_test_headers(dataframes_test)  "],"metadata":{"id":"sqFtQ36rNXBr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test_1.shape"],"metadata":{"id":"BdUg9XbFHQk3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print the first 5 rows of a dataset 'year1'\n","dataframes[3].shape"],"metadata":{"id":"n2e0PajXTNTe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data_preprocessiong\n"],"metadata":{"id":"PNpK_D74TrAU"}},{"cell_type":"code","source":["############################################################\n","# Get Clean dataframes by dropping all the rows which have missing values\n","def drop_nan_rows(dataframes, verbose=False):\n","    clean_dataframes = [df.dropna(axis=0, how='any') for df in dataframes]\n","    if verbose:\n","        for i in range(len(dataframes)):\n","            print(str(i+1)+'year:','Original Length=', len(dataframes[i]), '\\tCleaned Length=', len(clean_dataframes[i]), '\\tMissing Data=', len(dataframes[i])-len(clean_dataframes[i]))\n","    return clean_dataframes\n","\n","# Doing a quick analysis of how many missing values are there in each of the 5 dataframes\n","nan_dropped_dataframes = drop_nan_rows(dataframes, verbose=True)\n","#nan_dropped_dataframes = drop_nan_rows(dataframes_test, verbose=True)"],"metadata":{"id":"n7z7nZB5TiNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataframes[0].head()"],"metadata":{"id":"tlvsVdGSHIoc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def drop_missing(df):\n","  for i in range(len(df)):\n","    df[i].drop(columns=['X39', 'X23'], inplace=True)\n","  return df\n","dataframes = drop_missing(dataframes)\n","#test_frames = drop_missing(test_frames)"],"metadata":{"id":"BXhqGNtbHjrC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Data imputation*"],"metadata":{"id":"xOq8-29DWj7e"}},{"cell_type":"code","source":["#KNN\n","def perform_knn_imputation(dfs):\n","    knn_imputed_datasets = [fancyimpute.KNN(k=100,verbose=True).fit_transform(dfs[i]) for i in range(len(dfs))]\n","    return [pd.DataFrame(data=knn_imputed_datasets[i]) for i in range(len(dfs))]\n","knn_imputed_dataframes_test = perform_knn_imputation(dataframes_test)\n","knn_imputed_dataframes = perform_knn_imputation(dataframes)\n","set_new_headers(knn_imputed_dataframes)\n","set_test_headers(knn_imputed_dataframes_test)"],"metadata":{"id":"x0YokbIoWjJe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = knn_imputed_dataframes.copy()\n","test_frames = knn_imputed_dataframes_test.copy()"],"metadata":{"id":"eBgzjOQzWdq1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Handling Outlier\n"],"metadata":{"id":"c3OY2PdCYw5j"}},{"cell_type":"code","source":["train_index = []\n","for i in range(4):\n","  a = df[i][['X1','X2']]\n","  train_index.append(a)"],"metadata":{"id":"SDzwLo64xb4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_index= []\n","for i in range(4):\n","  b = df[i][['X1','X2']]\n","  test_index.append(b)"],"metadata":{"id":"eh3-qgDZUVXv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_index[0].shape"],"metadata":{"id":"E6vdzHcTHA5B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def refeature(dfs):\n","  for i in range(len(dfs)):\n","    #======================domain_feature============================\n","    dfs[i]['total_assets'] = dfs[i]['X57']/dfs[i]['X5']\n","    dfs[i]['ebit'] = dfs[i]['X9']*dfs[i]['total_assets']\n","    dfs[i]['sales'] = dfs[i]['X11']*dfs[i]['total_assets']\n","    dfs[i]['inventory'] = dfs[i]['sales']/dfs[i]['X62']\n","    dfs[i]['cost_of_product_sold'] = dfs[i]['inventory']*365/dfs[i]['X49']\n","    dfs[i]['current_liabilities'] = dfs[i]['cost_of_product_sold']*dfs[i]['X34']/365\n","    dfs[i]['retained_earnings'] = dfs[i]['total_assets']*dfs[i]['X8']\n","    dfs[i]['inventory_turnover'] = 365/dfs[i]['X49']\n","    dfs[i]['gross_profit'] = dfs[i]['X20']*dfs[i]['total_assets']\n","    dfs[i]['interest_cost'] = dfs[i]['ebit']-dfs[i]['gross_profit']\n","    dfs[i]['interest_cost_divided_to_sales'] = dfs[i]['interest_cost']/dfs[i]['sales']\n","    dfs[i]['equity'] = dfs[i]['total_assets']*dfs[i]['X12']\n","    dfs[i]['operating_profit'] = dfs[i]['X24']*dfs[i]['total_assets']\n","    dfs[i]['total_liabilities'] = dfs[i]['X4']*dfs[i]['total_assets']\n","    dfs[i]['market_value_of_equity'] = dfs[i]['X10']*dfs[i]['total_liabilities']\n","    dfs[i]['cash_flow']= dfs[i]['X28']*dfs[i]['total_liabilities']\n","    dfs[i]['current_assets'] = dfs[i]['X52']*dfs[i]['total_liabilities']\n","\n","\n","    #====================================classification_score=====================================\n","    #Springate1\n","    dfs[i]['springate_model'] = 1.03*dfs[i]['X5'] + 3.07*dfs[i]['X9']+0.66*(dfs[i]['ebit']/dfs[i]['current_liabilities'])+0.4*dfs[i]['X11']\n","    conditions1 =[(dfs[i]['springate_model']>=0.862), dfs[i]['springate_model']<0.862]\n","    values = [0,1]\n","    dfs[i]['bin_springate_model'] = np.select(conditions1, values)\n","    #Altman1\n","    dfs[i]['Altman_pattern'] = 0.717*(1/dfs[i]['X5'])+0.847*(1/dfs[i]['X8'])+3.108*(1/dfs[i]['X9'])+0.428*(dfs[i]['total_liabilities']/dfs[i]['market_value_of_equity'])+0.987*(1/dfs[i]['X38'])\n","    conditions2 = [(dfs[i]['Altman_pattern']>=1.81), dfs[i]['Altman_pattern']<1.81]\n","    values2 = [0,1]\n","    dfs[i]['bin_Altman_pattern'] = np.select(conditions2, values2)\n","    #Springate2\n","    dfs[i]['springate_model2'] = 1.03*(1/dfs[i]['X5']) + 3.07*dfs[i]['X9']+0.66*(dfs[i]['X28'])+0.4*dfs[i]['X11']\n","    conditions3 =[(dfs[i]['springate_model2']>=0.862), dfs[i]['springate_model2']<0.862]\n","    values3 = [0,1]\n","    dfs[i]['bin_springate_model2'] = np.select(conditions3, values3)\n","    #Zmijewski\n","    dfs[i]['zmijewski_pattern'] = -4.3 - 4.5*dfs[i]['X3']+5.7*dfs[i]['X4']-0.004*(dfs[i]['current_assets']/dfs[i]['current_liabilities'])\n","    conditions4 =[(dfs[i]['zmijewski_pattern']>0), dfs[i]['zmijewski_pattern']<=0]\n","    values4 = [1,0]\n","    dfs[i]['bin_zmijewski_pattern'] = np.select(conditions4, values4)\n","    #Gover Pattern\n","    dfs[i]['grover_pattern'] = 1.65*(1/dfs[i]['X5'])+3.404*(1/dfs[i]['X9'])-0.016*(1/dfs[i]['X3'])+0.057\n","    conditions5 =[(dfs[i]['grover_pattern']>=0.02), dfs[i]['grover_pattern']<0.02]\n","    values5 = [0,1]\n","    dfs[i]['bin_grover_pattern'] = np.select(conditions5, values5)\n","    #Fulmer pattern\n","    dfs[i]['fulmer_pattern']= 5.528*dfs[i]['X8']+0.212*dfs[i]['X11']+0.073*(dfs[i]['ebit']/dfs[i]['equity'])+1.207*dfs[i]['X28']-0.12*dfs[i]['X4']+2.335*(dfs[i]['current_liabilities']/dfs[i]['total_assets'])+0.575*dfs[i]['X31']+1.083*dfs[i]['X57']/dfs[i]['total_liabilities']+0.894*(dfs[i]['ebit']/dfs[i]['interest_cost'])- 6.075\n","    conditions6 =[(dfs[i]['fulmer_pattern']>=0), dfs[i]['fulmer_pattern']<0]\n","    values6 = [0,1]\n","    dfs[i]['bin_fulmer_pattern'] = np.select(conditions6, values6)\n","\n","\n","\n","    #=====================synthetic_feature======================================================\n","    dfs[i]['S_23'] = dfs[i]['X49']/dfs[i]['X29']\n","    dfs[i]['S_26'] = dfs[i]['X20']-dfs[i]['X36']\n","    dfs[i]['S_29'] = dfs[i]['X15']-dfs[i]['X48']\n","    dfs[i]['S_30'] = dfs[i]['X13']*dfs[i]['X46']\n","    dfs[i]['S_31'] = dfs[i]['X23']+dfs[i]['X64']\n","    dfs[i]['S_32'] = dfs[i]['X4']-dfs[i]['X47']\n","    dfs[i]['S_35'] = dfs[i]['X23']+dfs[i]['X64']\n","    dfs[i]['S_36'] = dfs[i]['X63']+dfs[i]['X23']\n","    dfs[i]['S_38'] = dfs[i]['X19']-dfs[i]['X7']\n","    #dfs[i]['S_41'] = dfs[i]['X64']-dfs[i]['X37']\n","  return dfs\n","clean_dataframes = refeature(df)\n","test_frames = refeature(test_frames)"],"metadata":{"id":"3bAqsFjjEeqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import preprocessing\n","scaler = preprocessing.RobustScaler()\n"],"metadata":{"id":"QKfMiQwEtKvw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def out_liers(df):\n","  for i in range(len(df)):\n","    df[i].drop(columns=['X1', 'X2'], inplace=True)\n","    col = df[i].columns\n","    df[i] = scaler.fit_transform(df[i])\n","    df[i] = pd.DataFrame(df[i], columns = col)\n","  for i in range(len(df)):\n","    for a in range(len(df)-1):\n","      df[a] = df[a].apply(lambda x: np.clip(x, a_min=np.quantile(x,q=0.02), a_max=np.quantile(x,q=0.98)))\n","  return df\n","clean_dataframes = out_liers(df)\n","test_frames = out_liers(test_frames)\n"],"metadata":{"id":"jZuB2rhajdO-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Poly"],"metadata":{"id":"v4eKKFJMgoAU"}},{"cell_type":"code","source":["imp_feat_list = ['X5','X31','X51','X14','X59','X42','X53','X36','X6','X48'] #  ,'DAYS_EMPLOYED'\n","poly = [a[imp_feat_list], b[imp_feat_list], c[imp_feat_list], d[imp_feat_list]]\n","poly_test = [a_test[imp_feat_list], b_test[imp_feat_list], c_test[imp_feat_list], d_test[imp_feat_list]]"],"metadata":{"id":"lvZMTZUrgqtK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import PolynomialFeatures\n","poly_transform = PolynomialFeatures(degree=2)\n","for i in range(4):\n","    poly_test[i] = poly_transform.fit_transform(poly_test[i])\n","    poly_feat_name_list = poly_transform.get_feature_names(imp_feat_list)\n","    poly_test[i] = pd.DataFrame(poly_test[i],columns=poly_feat_name_list)\n","\n","for i in range(4):\n","    poly[i] = poly_transform.fit_transform(poly[i])\n","    poly_feat_name_list = poly_transform.get_feature_names(imp_feat_list)\n","    poly[i] = pd.DataFrame(poly[i],columns=poly_feat_name_list)"],"metadata":{"id":"evGxf5lBg3Df"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(4):\n","    poly[i].index = poly[i].index\n","    clean_dataframes[i] = poly[i].merge(clean_dataframes[i],how='inner', on = imp_feat_list)\n","\n","for i in range(4):\n","    poly_test[i].index = poly_test[i].index\n","    test_frames[i] = poly_test[i].merge(test_frames[i],how='inner', on = imp_feat_list)"],"metadata":{"id":"Nfm5LJbohMPe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dealing with imbalanced data"],"metadata":{"id":"drnAAffwY4nE"}},{"cell_type":"code","source":["def check_data_imbalance(dfs):\n","    for i in range(len(dfs)):\n","        print('Dataset: '+str(i+1)+'year')\n","        print(dfs[i].groupby('Y').size())\n","        try:\n","          minority_percent = (dfs[i]['Y'].tolist().count(1) / len(dfs[i]['Y'].tolist()))*100\n","        except:\n","          minority_percent = 0 \n","        print('Minority (label 1) percentage: '+  str(minority_percent) + '%')\n","        print('-'*64)\n","        \n","check_data_imbalance(clean_dataframes)"],"metadata":{"id":"5Pl7gx4PY_hM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Feature with light gbm"],"metadata":{"id":"Ul7ym37X1kJh"}},{"cell_type":"code","source":["import lightgbm as lgb\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","\n","\n","\n","def plot_feature_importances(df, threshold = 0.9):\n","    \"\"\"\n","    Plots 15 most important features and the cumulative importance of features.\n","    Prints the number of features needed to reach threshold cumulative importance.\n","    \n","    Parameters\n","    --------\n","    df : dataframe\n","        Dataframe of feature importances. Columns must be feature and importance\n","    threshold : float, default = 0.9\n","        Threshold for prining information about cumulative importances\n","        \n","    Return\n","    --------\n","    df : dataframe\n","        Dataframe ordered by feature importances with a normalized column (sums to 1)\n","        and a cumulative importance column\n","    \n","    \"\"\"\n","    \n","    plt.rcParams['font.size'] = 18\n","    \n","    # Sort features according to importance\n","    df = df.sort_values('importance', ascending = False).reset_index()\n","    \n","    # Normalize the feature importances to add up to one\n","    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n","    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n","\n","    # Make a horizontal bar chart of feature importances\n","    plt.figure(figsize = (10, 6))\n","    ax = plt.subplot()\n","    \n","    # Need to reverse the index to plot most important on top\n","    ax.barh(list(reversed(list(df.index[:15]))), \n","            df['importance_normalized'].head(15), \n","            align = 'center', edgecolor = 'k')\n","    \n","    # Set the yticks and labels\n","    ax.set_yticks(list(reversed(list(df.index[:15]))))\n","    ax.set_yticklabels(df['feature'].head(15))\n","    \n","    # Plot labeling\n","    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n","    plt.show()\n","    \n","    # Cumulative importance plot\n","    plt.figure(figsize = (8, 6))\n","    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n","    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n","    plt.title('Cumulative Feature Importance');\n","    plt.show();\n","    \n","    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n","    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n","    \n","    return df\n","def identify_zero_importance_features(train, train_labels, iterations = 2):\n","# Initialize an empty array to hold feature importances\n","    feature_importances = np.zeros(train.shape[1])\n","\n","    # Create the model with several hyperparameters\n","    model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')\n","    \n","    # Fit the model multiple times to avoid overfitting\n","    for i in range(iterations):\n","\n","        # Split into training and validation set\n","        train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n","\n","        # Train using early stopping\n","        model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n","                  eval_metric = 'auc', verbose = 200)\n","\n","        # Record the feature importances\n","        feature_importances += model.feature_importances_ / iterations\n","    \n","    feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n","    \n","    # Find the features with zero importance\n","    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n","    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n","    \n","    return zero_features, feature_importances\n","\n","def feature_selection(df):\n","  model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 1000, class_weight = 'balanced')\n","  feature_dfs = df.drop(columns='Y')\n","  label_dfs = df.Y\n","  feat_names = feature_dfs.columns\n","  feature_importances = np.zeros(feature_dfs.shape[1])\n","\n","  for i in range(2):\n","    train_features, valid_features, train_y, valid_y = train_test_split(feature_dfs, label_dfs, test_size = 0.25, random_state = i)\n","    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n","              eval_metric = 'auc', verbose = 200)\n","    feature_importances += model.feature_importances_\n","  #feature_df\n","  feature_importances = feature_importances / 2\n","\n","  feature_importances = pd.DataFrame({'feature': feat_names, 'importance': feature_importances}).sort_values('importance', ascending = False)\n","  #zero_feature\n","  zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n","\n","  norm_feature_importances = plot_feature_importances(feature_importances)\n","\n","  df.drop(columns=zero_features, inplace=True)\n","\n","  norm_feature_importances = plot_feature_importances(feature_importances, threshold = 0.95)\n","  threshold = 0.99\n","\n","  # Extract the features to keep\n","  features_to_keep = list(norm_feature_importances[norm_feature_importances['cumulative_importance'] < threshold]['feature'])\n","  df_thin = df[features_to_keep]\n","  \n","  df_thin['Y'] = label_dfs\n","  return df_thin\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Pfq1WErV09jv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reshape_df(clean_dataframes):\n","  thin_df=[]\n","  for i in range(len(clean_dataframes)):\n","    a = feature_selection(clean_dataframes[i])\n","    thin_df.append(a)\n","  return thin_df\n","clean_dataframes = reshape_df(clean_dataframes=clean_dataframes)"],"metadata":{"id":"BoLTEUXN350Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = []\n","for i in range(len(test_frames)):\n","  a = clean_dataframes[i].drop(columns='Y')\n","  b = test_frames[i][a.columns]\n","  test.append(b)"],"metadata":{"id":"BzVHSbEOMNNV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_frames = test"],"metadata":{"id":"fa-iaF7JOGyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#df= clean_dataframes[3]\n","#df.to_csv('df_synthetic_4y.csv')"],"metadata":{"id":"GCkgTyqvPFrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from xgboost import XGBClassifier\n"],"metadata":{"id":"HQqLmlo2VjG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["param_a = {'booster': 'dart',\n"," 'lambda': 0.0003152874317077816,\n"," 'alpha': 0.041176956950987297,\n"," 'max_depth': 7,\n"," 'eta': 0.9825770244495555,\n"," 'gamma': 0.8781445883047275,\n"," 'grow_policy': 'lossguide',\n"," 'sample_type': 'uniform',\n"," 'normalize_type': 'tree',\n"," 'rate_drop': 0.07516367633155714,\n"," 'skip_drop': 0.00012781727831260767}\n","param_b = {'booster': 'gbtree',\n"," 'lambda': 0.7453988037016246,\n"," 'alpha': 1.7182754110102324e-07,\n"," 'max_depth': 9,\n"," 'eta': 0.6930606999370981,\n"," 'gamma': 0.010731654947300806,\n"," 'grow_policy': 'depthwise'}\n","param_c = {'booster': 'dart',\n"," 'lambda': 2.0184275036888347e-06,\n"," 'alpha': 0.01361968260753759,\n"," 'max_depth': 9,\n"," 'eta': 0.7079474373774491,\n"," 'gamma': 1.938059463499507e-07,\n"," 'grow_policy': 'lossguide',\n"," 'sample_type': 'uniform',\n"," 'normalize_type': 'tree',\n"," 'rate_drop': 0.0002843621852759622,\n"," 'skip_drop': 1.8323176726706248e-07}\n","\n","param_d = {'booster': 'dart',\n"," 'lambda': 2.0415476800032797e-08,\n"," 'alpha': 0.00158681936909514,\n"," 'max_depth': 9,\n"," 'eta': 0.6675522927486546,\n"," 'gamma': 9.22087534202792e-07,\n"," 'grow_policy': 'depthwise',\n"," 'sample_type': 'weighted',\n"," 'normalize_type': 'tree',\n"," 'rate_drop': 0.035899546173585185,\n"," 'skip_drop': 1.283206678881014e-07}\n","param = [param_a, param_b, param_c, param_d]"],"metadata":{"id":"fR59T2Zsbq4E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_dataframes_features_labels(dfs):\n","    feature_dfs = [dfs[i].drop(columns='Y') for i in range(len(dfs))]\n","    label_dfs = [dfs[i].Y for i in range(len(dfs))]\n","    return feature_dfs, label_dfs\n","\n","from sklearn.preprocessing import RobustScaler\n","scaler = RobustScaler()"],"metadata":{"id":"h2_a7M95tKV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"2Iut6Jk6dh1y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_dfs, label_dfs = split_dataframes_features_labels(clean_dataframes)   \n"],"metadata":{"id":"4k3mYzDj1kxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["smote = SMOTE(sampling_strategy='auto' , random_state=42, k_neighbors=10)"],"metadata":{"id":"tha2GLUYkLee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PCA\n"],"metadata":{"id":"XHS2FoIUzCXG"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=30)"],"metadata":{"id":"KTTHdWsszFF9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run on one year"],"metadata":{"id":"TfzD3jst1qfB"}},{"cell_type":"code","source":["X_train, X_test ,y_train, y_test = train_test_split(feature_dfs[0], label_dfs[0], test_size=.2)\n","\n","ratio = float(np.sum(y_train == 0)) / np.sum(y_train==1)\n","#X_train = pca.fit_transform(X_train)\n","#X_test = pca.fit_transform(X_test)\n","#test_frames[0] = pca.fit_transform(test_frames[0])\n","#X_train, y_train =  smote.fit_resample(X_train, y_train)\n","clf = XGBClassifier(**param[0],scale_pos_weight=ratio)              \n","clf.fit(X_train, y_train, verbose=True, early_stopping_rounds=1000, eval_metric='logloss', eval_set=[[X_test,y_test]])\n","y_test_predicted = clf.predict(test_frames[0])\n","del X_train, X_test ,y_train, y_test, clf, ratio\n"],"metadata":{"id":"KLsxFmZ81qIg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test ,y_train, y_test = train_test_split(feature_dfs[1], label_dfs[1], test_size=.2)\n","ratio = float(np.sum(y_train == 0)) / np.sum(y_train==1)\n","#X_train, y_train =  smote.fit_resample(X_train, y_train)\n","#X_train = pca.fit_transform(X_train)\n","#X_test = pca.fit_transform(X_test)\n","#test_frames[1] = pca.fit_transform(test_frames[1])\n","clf = XGBClassifier(**param[1], scale_pos_weight=ratio)              \n","clf.fit(X_train, y_train, verbose=True, early_stopping_rounds=1000, eval_metric='logloss', eval_set=[[X_test,y_test]])\n","y_test_predicted2 = clf.predict(test_frames[1])\n","del X_train, X_test ,y_train, y_test, clf,ratio"],"metadata":{"id":"HiuBm5yR1ojd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test ,y_train, y_test = train_test_split(feature_dfs[2], label_dfs[2], test_size=.2) \n","ratio = float(np.sum(y_train == 0)) / np.sum(y_train==1)\n","#X_train, y_train =  smote.fit_resample(X_train, y_train)\n","#X_train = pca.fit_transform(X_train)\n","#X_test = pca.fit_transform(X_test)\n","#test_frames[2] = pca.fit_transform(test_frames[2])\n","clf = XGBClassifier(**param[2], scale_pos_weight=ratio)              \n","clf.fit(X_train, y_train, verbose=True, early_stopping_rounds=1000, eval_metric='logloss', eval_set=[[X_test,y_test]])\n","y_test_predicted3 = clf.predict(test_frames[2])\n","del X_train, X_test ,y_train, y_test, clf"],"metadata":{"id":"ehqWkzZBAhgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test ,y_train, y_test = train_test_split(feature_dfs[3], label_dfs[3], test_size=.2)\n","ratio = float(np.sum(y_train == 0)) / np.sum(y_train==1)\n","#X_train, y_train =  smote.fit_resample(X_train, y_train)\n","#X_train = pca.fit_transform(X_train)\n","#X_test = pca.fit_transform(X_test)\n","#test_frames[3] = pca.fit_transform(test_frames[3])\n","clf = XGBClassifier(**param[3], scale_pos_weight=ratio)              \n","clf.fit(X_train, y_train, verbose=True, early_stopping_rounds=1000, eval_metric='logloss', eval_set=[[X_test,y_test]])\n","y_test_predicted4 = clf.predict(test_frames[3])\n","del X_train, X_test ,y_train, y_test, clf"],"metadata":{"id":"PP-SPIfAAyAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_label = [y_test_predicted, y_test_predicted2, y_test_predicted3, y_test_predicted4]"],"metadata":{"id":"TUTSHm3tA_09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(test_label[0])"],"metadata":{"id":"KG8VGMKLDG8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(test_index[0])"],"metadata":{"id":"vuqwV1_hDere"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"Pv285o68N3lo"}},{"cell_type":"code","source":["df_test_1['class'] = test_label[0]\n","df_test_2['class'] = test_label[1]\n","df_test_3['class'] = test_label[2]\n","df_test_4['class'] = test_label[3]\n"],"metadata":{"id":"2d6tptwaCA1H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = pd.concat([df_test_1, df_test_2, df_test_3, df_test_4])"],"metadata":{"id":"8-IfImM6JYd3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.set_index('X1').sort_index()"],"metadata":{"id":"NjzSrw7yK0Yr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x['id']= x['X1']"],"metadata":{"id":"wNEwckapLtQm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out_put = x[['id','class']]\n"],"metadata":{"id":"5D0AXm-qL4k5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out_put['class'].value_counts()"],"metadata":{"id":"BUif-hWi4Nrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out_put.to_csv('out0601_1345.csv')"],"metadata":{"id":"lG0PP6t3if5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test_1.shape"],"metadata":{"id":"93AULztTHgDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_label[0]"],"metadata":{"id":"OQ-9js73BQ76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pandas.core.common import random_state\n","feature_dfs, label_dfs = split_dataframes_features_labels(clean_dataframes)   \n","verbose= False\n","k_folds = 5\n","result = []\n","for df_index in range(len(clean_dataframes)):\n","  if verbose: print('\\t\\tDataset: ' + '\\033[1m' + str(df_index+1) + 'year' + '\\033[0m')\n","                \n","  # Calling the 'prepare_kfold_cv_data' returns lists of features and labels \n","  # for train and test sets respectively.\n","  # The number of items in the list is equal to k_folds\n","  X_train_list, y_train_list, X_test_list, y_test_list = train_test_split(feature_dfs[df_index], label_dfs[df_index], test_size=.2)             \n","  clf = XGBClassifier(**param[df_index])              \n","  clf.fit(X_train_list, y_train_list, verbose=True, early_stopping_rounds=1000, eval_metric='logloss', eval_set=[[X_test_list,y_test_list]])\n","  y_test_predicted = clf.predict(test_frames[df_index])\n","  result.append(y_test_predicted)\n","                    \n","    "],"metadata":{"id":"kjhk7RwwMZTg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"tk61L3XNM3D_"},"execution_count":null,"outputs":[]}]}