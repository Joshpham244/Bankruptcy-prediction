{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"XGB Bankcruptcy 27042022.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["3kRHfQ9A5w1a"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HZWsWCYpe5BG"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd /content/drive/My Drive/newdata"],"metadata":{"id":"ja8sRxQ_fA9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import sys\n","import seaborn as sns\n","import scipy as sp\n","import numpy as np\n","import shelve\n","from sklearn import preprocessing\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report"],"metadata":{"id":"u6r82jvjfJ8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv('/content/drive/MyDrive/newdata/train_bankcruptcy.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/newdata/test_bankcruptcy.csv')"],"metadata":{"id":"ljHHVt39fM-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.head()"],"metadata":{"id":"F6jzdbnxfrDQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_cat_var(df):\n","\n","    cat_colnames = list(df.select_dtypes('object').columns)\n","\n","    for col in cat_colnames:\n","      print('Categorical column: {}\\n'.format(col))\n","      print('Number of unique entries: {}\\n'.format(df[col].nunique()))\n","      print('Unique entry names:\\n{}\\n'.format(df[col].unique()))\n","      print('Value counts of each entry:\\n{}\\n'.format(df[col].value_counts(dropna=False)))\n","      print('---------------------------------------------------------')\n","\n","    return"],"metadata":{"id":"9V8vVQ-uftIq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# modules"],"metadata":{"id":"3kRHfQ9A5w1a"}},{"cell_type":"code","source":["def plot_kde_hist_var(df,varList,calcStat = True, drawAll = False):\n","    numVar = len(varList)\n","\n","    plt.figure(figsize=(10,numVar*4))\n","    ks_stat_list = []\n","    ks_pval_list = []\n","    try:\n","        for i,var in enumerate(varList):\n","            tgt_true = df.loc[df['TARGET']==1,var]\n","            tgt_false = df.loc[df['TARGET']==0,var]\n","\n","            # calculate statistical significance between both populations\n","            if calcStat == True:\n","                (ks_stat,ks_pval)= sp.stats.ks_2samp(tgt_true,tgt_false)\n","                ks_stat_list.append(ks_stat)\n","                ks_pval_list.append(ks_pval)\n","                ks_hval_list = [True for hyp in ks_pval_list if hyp<0.05]\n","\n","            #\n","            median_tgt_true = tgt_true.median()\n","            median_tgt_false = tgt_false.median()\n","            corrVal = df['TARGET'].corr(df[var])\n","            print('Median Value of {} when Target (True): {:.6f}'.format(var,median_tgt_true))\n","            print('Median Value of {} when Target (False): {:.6f}'.format(var,median_tgt_false))\n","            print('Pearson Correlation of {} with Target (True): {:.6f}'.format(var,corrVal))\n","\n","            # drawing KDE distributions\n","            tgt_true.dropna(inplace=True) # require to dropna for sns.distplot function\n","            tgt_false.dropna(inplace=True)\n","            plt.subplot(numVar,1,i+1)\n","            sns.distplot(tgt_true,rug=drawAll,kde=drawAll,label='Target: True')\n","            sns.distplot(tgt_false,rug=drawAll,kde=drawAll,label='Target: False')\n","            plt.legend()\n","            #plt.title(var)\n","    except TypeError as error:\n","        print(error)\n","        print('Features are objects.  Need ints/floats')\n","\n","    return ks_hval_list, ks_pval_list\n","def label_encoding_df(df,cat_limit = 2):\n","\n","    le = preprocessing.LabelEncoder()\n","    le_count = 0\n","    label_encode_list = []\n","    for col in df:\n","        if df[col].dtype=='object':\n","            if df[col].nunique(dropna=False) <= cat_limit:\n","                print(col)\n","                le_count += 1\n","                le.fit(df[col])\n","                df[col] = le.transform(df[col])\n","                label_encode_list.append(col)\n","\n","    print('{0} columns were label encoded'.format(le_count))\n","\n","    return df, label_encode_list\n","def print_tab_miss_val(df,miss_val_thresh=50,numColPrint=10,printData=False):\n","    # Evaluate missing values in the data\n","    num_miss_val = df.isnull().sum()\n","    pct_miss_val = num_miss_val/df.shape[0]*100\n","\n","    tab_miss_val = pd.concat([num_miss_val,pct_miss_val],axis=1)\n","    tab_miss_val.columns = ['Missing Values','Percentage']\n","    tab_miss_val  = tab_miss_val[tab_miss_val['Missing Values']>0]\n","    tab_miss_val['Percentage'] = tab_miss_val['Percentage'].round(1)\n","    tab_miss_val.sort_values(['Percentage'],ascending=False,inplace=True)\n","\n","    numCol_miss_val = tab_miss_val.shape[0]\n","    numCol_total = df.shape[1]\n","    pctCol_miss_val = round((numCol_miss_val/numCol_total)*100)\n","\n","    numCol_crit_miss_val = tab_miss_val[tab_miss_val['Percentage'] > miss_val_thresh].shape[0]\n","    pctCol_crit_miss_val = round(numCol_crit_miss_val/numCol_total*100)\n","\n","    info_miss_val = pd.Series(data=[numCol_miss_val,pctCol_miss_val,numCol_crit_miss_val,pctCol_crit_miss_val],\n","              index=['Cols Missing Values','Cols Missing Values (%)',\n","            'Cols Critical Missing Values', 'Cols Critical Missing Values (%)'])\n","\n","    if printData==True:\n","        print(info_miss_val)\n","        print('\\n Top {} columns with missing values is as follows:'.format(numColPrint))\n","        print(tab_miss_val['Percentage'].head(numColPrint))\n","        \n","\n","    return info_miss_val, tab_miss_val, numColPrint \n","def convSeries2Str(seriesData):\n","    strList = ''\n","    for idx,val in seriesData.iteritems():\n","        strVal = '{}({}), '.format(idx,val)\n","        strList = strList + strVal\n","\n","    return strList\n","def print_basic_info_df(df,bal_thresh=30):\n","\n","    (numRow,numCol) = df.shape\n","    memory = int(sys.getsizeof(df)/(10**6))\n","\n","    dtypeVals = df.dtypes.value_counts()\n","    dtypeStr = convSeries2Str(dtypeVals)\n","\n","    # Extract the unique variables of each  column that are strings, and extract the unique variables including NaNs\n","    catVals = df.select_dtypes('object').nunique(dropna=False)\n","    catStr = convSeries2Str(catVals)\n","\n","    # Is the dataframe balanced?\n","    if 'TARGET' in df:\n","        (numRow,numCol) = df.shape\n","        pctTarget_true = int(df['TARGET'].sum()/numRow*100)\n","        if pctTarget_true > 100-bal_thresh or pctTarget_true < bal_thresh:\n","            isBalanced='No'\n","        else:\n","            isBalanced='True'\n","    else:\n","        isBalanced='N/A'\n","        pctTarget_true='N/A'\n","\n","    series_data = [numRow, numCol, dtypeStr,memory,pctTarget_true,isBalanced,catStr]\n","    series_idx = ['Num rows','Num cols','Dtype','Memory (MB)','True (%)','Is Balanced','Categorical cols']\n","    series_info = pd.Series(series_data,index = series_idx)\n","\n","    dict_info = [{'Num rows': numRow, 'Num cols': numCol,'Dtype': dtypeStr,\n","    'Memory (MB)': memory,'True (%)': pctTarget_true,'Is Balanced':isBalanced,\n","    'Category cols': catStr} ]\n","\n","    return series_info\n","def print_compare_df(df1,df2,miss_val_thresh=50,bal_thresh=30,printCompareData=False):\n","\n","    # Prints combined basic data of each dataframe\n","    df1_basicinfo = print_basic_info_df(df1)\n","    df2_basicinfo = print_basic_info_df(df2)\n","    comb_basic_info = pd.concat([df1_basicinfo,df2_basicinfo],axis=1)\n","\n","    # Compare missing value data\n","    miss_val_info_df1, miss_val_tab_df1 =  print_tab_miss_val(df1)\n","    miss_val_info_df2, miss_val_tab_df2 =  print_tab_miss_val(df2)\n","    comb_miss_val_info = pd.concat([miss_val_info_df1,miss_val_info_df2],axis=1)\n","\n","\n","    s1 = set(df1.dtypes)\n","    s2 = set(df2.dtypes)\n","\n","    # Compare two dataframes for number of missing categories, and values in each category\n","    # As the training and test datasets are of different sizes, the training dataset may have values\n","    # in the feature columns that are not in the test datasets.\n","    # This code analyzes whether there are more than 5 different unique variables between feature columns\n","    # of the test and training datasets.\n","    if s1 == s2:\n","        for x in list(s1):\n","\n","            df1_catCols = df1.select_dtypes(x).nunique(dropna=False)\n","            df2_catCols = df2.select_dtypes(x).nunique(dropna=False)\n","            diff_catColsList = df1_catCols - df2_catCols\n","            diff_catCols = diff_catColsList[(diff_catColsList<5) & (diff_catColsList>-5) & (diff_catColsList!=0)]\n","            for y in diff_catCols.index:\n","                df1_valCnt = df1[y].value_counts()\n","                df1_valCnt.name = df1_valCnt.name+'_DF1'\n","                df2_valCnt = df2[y].value_counts()\n","                df2_valCnt.name = df2_valCnt.name+'_DF2'\n","                comb_valCnt = pd.concat([df1_valCnt,df2_valCnt],axis=1)\n","\n","                if printCompareData==True:\n","                    print(comb_valCnt)\n","                    plt.figure()\n","                    comb_valCnt.plot.bar(rot=60,title=y)\n","\n","\n","\n","    return comb_basic_info, comb_miss_val_info, miss_val_tab_df1, miss_val_tab_df2"],"metadata":{"id":"GM04AC7W0_HQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocess"],"metadata":{"id":"gBE-gcRr54PN"}},{"cell_type":"code","source":["df_train = df_train.replace({'?':np.nan})\n","df_test = df_test.replace({'?':np.nan})\n","df_train = df_train.rename(columns={\"class\":\"TARGET\"})"],"metadata":{"id":"0mmBZ0uggkHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in df_train.columns:\n","  df_train[i] = pd.to_numeric(df_train[i])\n","  "],"metadata":{"id":"CKig-uRt7GgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_tab_miss_val1(df,miss_val_thresh=50,numColPrint=10,printData=False):\n","    # Evaluate missing values in the data\n","    num_miss_val = df.isnull().sum()\n","    pct_miss_val = num_miss_val/df.shape[0]*100\n","\n","    tab_miss_val = pd.concat([num_miss_val,pct_miss_val],axis=1)\n","    tab_miss_val.columns = ['Missing Values','Percentage']\n","    tab_miss_val  = tab_miss_val[tab_miss_val['Missing Values']>0]\n","    tab_miss_val['Percentage'] = tab_miss_val['Percentage'].round(1)\n","    tab_miss_val.sort_values(['Percentage'],ascending=False,inplace=True)\n","\n","    numCol_miss_val = tab_miss_val.shape[0]\n","    numCol_total = df.shape[1]\n","    pctCol_miss_val = round((numCol_miss_val/numCol_total)*100)\n","\n","    numCol_crit_miss_val = tab_miss_val[tab_miss_val['Percentage'] > miss_val_thresh].shape[0]\n","    pctCol_crit_miss_val = round(numCol_crit_miss_val/numCol_total*100)\n","\n","    info_miss_val = pd.Series(data=[numCol_miss_val,pctCol_miss_val,numCol_crit_miss_val,pctCol_crit_miss_val],\n","              index=['Cols Missing Values','Cols Missing Values (%)',\n","            'Cols Critical Missing Values', 'Cols Critical Missing Values (%)'])\n","\n","    if printData==True:\n","        print(info_miss_val)\n","        print('\\n Top {} columns with missing values is as follows:'.format(numColPrint))\n","        print(tab_miss_val['Percentage'].head(numColPrint))\n","        \n","\n","    return tab_miss_val\n"],"metadata":{"id":"OyFuHDeM9J8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extract_cat_var(df_train)"],"metadata":{"id":"aLtK8J1Q6Qjg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['total_liabilities_minus_cash_over_sales'].describe()"],"metadata":{"id":"-YgrQX8YNXql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["varList = df_train.columns[:].values\n","hVal_list,pVal_list= plot_kde_hist_var(df_train,varList)"],"metadata":{"id":"KOmO9wF1436_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_corr = df_train.corr()\n","df_app_train_corr_target = df_train_corr['TARGET'].sort_values()\n","print('+ve corr: \\n{0}'.format(df_app_train_corr_target.tail(32)))\n","print('-ve corr: \\n{0}'.format(df_app_train_corr_target.head(32)))"],"metadata":{"id":"ZH1Sjhrq9YjU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["var_pos_corr_list = df_app_train_corr_target.head(10).index.values\n","var_neg_corr_list = df_app_train_corr_target[-2:-10:-1].index.values\n","\n","print(var_pos_corr_list)\n","print(var_neg_corr_list)\n","\n","#plot_kde_hist_var(df_train,var_pos_corr_list,drawAll=True)\n","plot_kde_hist_var(df_train,var_neg_corr_list,drawAll=True)"],"metadata":{"id":"z_nAHvLb9nk7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['long_term_liabilities_over_equity'].hist()"],"metadata":{"id":"k7XXGUoNCA7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['total_assets'] = np.exp(df_train['logarithm_of_total_assets'])"],"metadata":{"id":"k_f5syQND6t3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.head()"],"metadata":{"id":"IAZyuBkHEvQk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['total_assets'] = pd.to_numeric(df_train['total_assets'])"],"metadata":{"id":"SP5qEBwEE44d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['total_assets'].describe()"],"metadata":{"id":"kaV1gXraFeyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['equity'] = df_train['equity_over_total_assets']*df_train['total_assets']"],"metadata":{"id":"3iE6a6ZcGZQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['long_term_liabilities'] = df_train['long_term_liabilities_over_equity']*df_train['equity']"],"metadata":{"id":"lgJpRiXeG3kb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"sZx8foo0HgMj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["varList = ['long_term_liabilities']\n","hVal_list,pVal_list= plot_kde_hist_var(df_train,varList)"],"metadata":{"id":"5AOQotIzHgaz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["check = df_train['long_term_liabilities'] == 0\n","df = df_train[check]\n","len(df)"],"metadata":{"id":"mh7nhoEQHJzc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['short_term_liabilities'] = df_train['short_term_liabilities_over_total_assets'] * df_train['total_assets']"],"metadata":{"id":"AxpFV9AvH6R9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#df['current_assets_minus_inventories_over_long_term_liabilities'] = (df_train['current_assets_minus_inventories_over_short_term_liabilities'] * df_train['short_term_liabilities'])/df['long_term_liabilities']"],"metadata":{"id":"UrAzBD1RIoYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["condi1 = df_train['total_liabilities_over_total_assets'] <1\n","condi2 = df_train['total_liabilities_over_total_assets'] > 0\n","condi3 = df_train['total_liabilities_over_total_assets'] = 1\n","df_train = df_train[(condi1|condi3)&condi2]"],"metadata":{"id":"3BjF2XOgKzEu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#['TARGET','logarithm_of_total_assets','working_capital_over_total_assets','retained_earnings_over_total_assets','profit_on_sales_over_sales']"],"metadata":{"id":"n6mw-7jzN3-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_ext_src = df_train[['total_liabilities_over_total_assets','working_capital_over_total_assets', 'retained_earnings_over_total_assets', 'TARGET']] \n","df_ext_src_corr = df_ext_src.corr()\n","sns.heatmap(df_ext_src_corr,vmin=-1.0,vmax=1.0,annot=True)\n","#sns.set(rc={'figure.figsize':(160.7,80.27)})"],"metadata":{"id":"c5g5CDXa9n40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train[['EBIT_over_total_assets', 'gross_profit_plus_interest_over_total_assets']].describe()"],"metadata":{"id":"YAeoAzWuSVFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["var_list = ['total_liabilities_over_total_assets']"],"metadata":{"id":"EqyJ4M4zAOpd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.drop(columns=['total_assets_over_total_liabilities'],inplace=True)"],"metadata":{"id":"7o7UjKGbOFWE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['sales_over_fixed_assets'].describe()"],"metadata":{"id":"TVjDQTgk9oKt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['net_profit_over_total_assets'].hist()"],"metadata":{"id":"CqHRy8JCQL8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['total_liabilities_over_total_assets'].hist()"],"metadata":{"id":"wiwC5OCwej3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = df_train.copy()\n","\n","train_labels = train['TARGET']\n","train = train.drop(columns=['TARGET'])\n","\n","X = train\n","Y = train_labels\n","feat_names = list(X.columns)\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2,\n","                                                    random_state=0)\n"],"metadata":{"id":"QarS_m7qMvP8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","imputer = SimpleImputer(missing_values=np.nan,strategy='median')\n","\n","from sklearn.preprocessing import RobustScaler\n","scaler = RobustScaler()"],"metadata":{"id":"ha9tTErTj0dU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imputer.fit(train)\n","X_train = imputer.transform(X_train)\n","X_test = imputer.transform(X_test)\n","\n","X2_test = X_test.copy()\n","\n","scaler.fit(train)\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"T-PkIWWXkjFd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","import xgboost as xgb"],"metadata":{"id":"rmWqkGYZkr54"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf = XGBClassifier(objective='binary:logistic',seed=42,scale_pos_weight=2.2775)"],"metadata":{"id":"0OqcCXrEkwx9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf.fit(X_train, Y_train, verbose=True, early_stopping_rounds=10, eval_metric='aucpr', eval_set=[[X_test,Y_test]])"],"metadata":{"id":"eiOcykLmkzCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install scikit-plot\n"],"metadata":{"id":"-A8fmbpwlAj6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scikitplot.metrics import plot_roc, plot_precision_recall\n","y_score = clf.fit(X_train, Y_train).predict_proba(X_test)\n","\n","\n","# Plot metrics \n","plot_roc(Y_test, y_score)\n","plt.show()\n","    \n","plot_precision_recall(Y_test, y_score)\n","plt.show()"],"metadata":{"id":"lDXJf1DAlLQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"-PYf0sgtMjj7"},"execution_count":null,"outputs":[]}]}