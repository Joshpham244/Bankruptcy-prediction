{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bản sao của Preprocess_3042022.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HZWsWCYpe5BG"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd /content/drive/My Drive/newdata"],"metadata":{"id":"ja8sRxQ_fA9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install fancyimpute\n","!pip install impyute"],"metadata":{"id":"6RVHbVKpLpf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To supress warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","# Basic Libraries for Data organization, Statistical operations and Plotting\n","import numpy as np\n","import pandas as pd\n","%matplotlib inline\n","# For loading .arff files\n","from scipy.io import arff\n","# To analyze the type of missing data\n","import missingno as msno\n","# Library for performing k-NN and MICE imputations \n","import fancyimpute\n","# Library to perform Expectation-Maximization (EM) imputation\n","import impyute as impy\n","# To perform mean imputation\n","#To perform kFold Cross Validation\n","from sklearn.model_selection import KFold\n","# Formatted counter of class labels\n","from collections import Counter\n","# Ordered Dictionary\n","from collections import OrderedDict\n","# Library imbalanced-learn to deal with the data imbalance. To use SMOTE oversampling\n","from imblearn.over_sampling import SMOTE \n","from scipy.stats.mstats import winsorize\n","\n","\n","# Impoting classification models\n","from xgboost import XGBClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from imblearn.ensemble import BalancedBaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","import random\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import precision_recall_curve"],"metadata":{"id":"u6r82jvjfJ8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv('/content/drive/MyDrive/newdata/train_bankcruptcy.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/newdata/test_bankcruptcy.csv')"],"metadata":{"id":"ljHHVt39fM-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.head()"],"metadata":{"id":"F6jzdbnxfrDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Module"],"metadata":{"id":"7K8nH893MIHa"}},{"cell_type":"code","source":["def load_dataframes(a):\n","    return [pd.DataFrame(data_i_year[0]) for data_i_year in a]\n","def set_new_headers(dataframes):\n","    cols = ['X' + str(i+1) for i in range(len(dataframes[0].columns)-1)]\n","    cols.append('Y')\n","    for df in dataframes:\n","        df.columns = cols\n"],"metadata":{"id":"lVzfpG7LMIlp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocess"],"metadata":{"id":"gBE-gcRr54PN"}},{"cell_type":"code","source":["df_train = df_train.replace({'?':np.nan})\n","df_test = df_test.replace({'?':np.nan})\n","df_train = df_train.rename(columns={\"class\":\"TARGET\"})"],"metadata":{"id":"0mmBZ0uggkHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test.shape"],"metadata":{"id":"eINyKEJrd4kB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index = 0 \n","while(index<=65):\n","  colname = df_train.columns[index]\n","  col = getattr(df_train, colname)\n","  df_train[colname] = col.astype(float)\n","  index+=1"],"metadata":{"id":"CKig-uRt7GgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index = 0 \n","while(index<=65):\n","  colname = df_test.columns[index]\n","  col = getattr(df_test, colname)\n","  df_test[colname] = col.astype(float)\n","  index+=1"],"metadata":{"id":"l2YP0TVqNOEY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_1 = df_train[df_train['forecasting period']==1]\n","df_train_2 = df_train[df_train['forecasting period']==2]\n","df_train_3 = df_train[df_train['forecasting period']==3]\n","df_train_4 = df_train[df_train['forecasting period']==4]\n","dataframes = [df_train_1, df_train_2, df_train_3, df_train_4]\n","set_new_headers(dataframes)    \n"],"metadata":{"id":"MCednIdrOoe3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test_1 = df_test[df_test['forecasting period']==1]\n","df_test_2 = df_test[df_test['forecasting period']==2]\n","df_test_3 = df_test[df_test['forecasting period']==3]\n","df_test_4 = df_test[df_test['forecasting period']==4]\n","dataframes_test = [df_test_1, df_test_2, df_test_3, df_test_4]\n","set_new_headers(dataframes_test)  "],"metadata":{"id":"sqFtQ36rNXBr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print the first 5 rows of a dataset 'year1'\n","dataframes[3].Y.describe()"],"metadata":{"id":"n2e0PajXTNTe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data_preprocessiong\n"],"metadata":{"id":"PNpK_D74TrAU"}},{"cell_type":"code","source":["############################################################\n","# Get Clean dataframes by dropping all the rows which have missing values\n","def drop_nan_rows(dataframes, verbose=False):\n","    clean_dataframes = [df.dropna(axis=0, how='any') for df in dataframes]\n","    if verbose:\n","        for i in range(len(dataframes)):\n","            print(str(i+1)+'year:','Original Length=', len(dataframes[i]), '\\tCleaned Length=', len(clean_dataframes[i]), '\\tMissing Data=', len(dataframes[i])-len(clean_dataframes[i]))\n","    return clean_dataframes\n","\n","# Doing a quick analysis of how many missing values are there in each of the 5 dataframes\n","nan_dropped_dataframes = drop_nan_rows(dataframes, verbose=True)\n","nan_dropped_dataframes = drop_nan_rows(dataframes_test, verbose=True)"],"metadata":{"id":"n7z7nZB5TiNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate the sparsity matrix (figure) for all the dataframes\n","def generate_sparsity_matrix(dfs):\n","    for i in range(4):\n","        missing_df_i = dfs[i].columns[dfs[i].isnull().any()].tolist()\n","        msno.matrix(dfs[i][missing_df_i], figsize=(20,5))\n","\n","generate_sparsity_matrix(dataframes)"],"metadata":{"id":"qsMhuJXeTUse"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate the heatmap for all the dataframes\n","def generate_heatmap(dfs):\n","    for i in range(4):\n","        missing_df_i = dfs[i].columns[dfs[i].isnull().any()].tolist()\n","        msno.heatmap(dfs[i][missing_df_i], figsize=(20,20))\n","        \n","generate_heatmap(dataframes)  "],"metadata":{"id":"5FDMKYsXWDQ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Data imputation*"],"metadata":{"id":"xOq8-29DWj7e"}},{"cell_type":"code","source":["#KNN\n","def perform_knn_imputation(dfs):\n","    knn_imputed_datasets = [fancyimpute.KNN(k=100,verbose=True).fit_transform(dfs[i]) for i in range(len(dfs))]\n","    return [pd.DataFrame(data=knn_imputed_datasets[i]) for i in range(len(dfs))]\n","knn_imputed_dataframes_test = perform_knn_imputation(dataframes_test)\n","knn_imputed_dataframes = perform_knn_imputation(dataframes)\n","set_new_headers(knn_imputed_dataframes)\n","set_new_headers(knn_imputed_dataframes_test)"],"metadata":{"id":"x0YokbIoWjJe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = knn_imputed_dataframes\n","test_frames = knn_imputed_dataframes_test"],"metadata":{"id":"eBgzjOQzWdq1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Handling Outlier\n"],"metadata":{"id":"c3OY2PdCYw5j"}},{"cell_type":"code","source":["train_index = []\n","for i in range(3):\n","  a = df[i][['X1','X2']]\n","  train_index.append(a)"],"metadata":{"id":"SDzwLo64xb4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_index= []\n","for i in range(3):\n","  a = df[i][['X1','X2']]\n","  test_index.append(a)"],"metadata":{"id":"eh3-qgDZUVXv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import preprocessing\n"],"metadata":{"id":"QKfMiQwEtKvw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaler = preprocessing.RobustScaler()\n","for i in range(4):\n","  df[i].drop(columns=['X1', 'X2'], inplace=True)\n","  col = df[i].columns\n","  df[i] = scaler.fit_transform(df[i])\n","  df[i] = pd.DataFrame(df[i], columns = col)"],"metadata":{"id":"mrm0fTmNsnxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(4):\n","  test_frames[i].drop(columns=['X1', 'X2'], inplace=True)\n","  col = test_frames[i].columns\n","  test_frames[i] = scaler.fit_transform(test_frames[i])\n","  test_frames[i] = pd.DataFrame(test_frames[i], columns = col)"],"metadata":{"id":"J3-7QpEkPKHL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# train"],"metadata":{"id":"UkPqJkDNPknL"}},{"cell_type":"code","source":["a=df[0]\n","for i in a.columns:\n","  if i == ['Y']:\n","    a[i] = a[i]\n","  else:\n","    a[i] = a[i].apply(lambda x: np.clip(x, a_min=np.quantile(x,q=0.02), a_max=np.quantile(x,q=0.98)))"],"metadata":{"id":"7RFYpx2v095g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b=df[1]\n","for i in a.columns:\n","  if i == ['Y']:\n","    b[i] = b[i]\n","  else:\n","    b[i] = b[i].apply(lambda x: np.clip(x, a_min=np.quantile(x,q=0.02), a_max=np.quantile(x,q=0.98)))"],"metadata":{"id":"HrKmlThSXUnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c=df[1]\n","for i in a.columns:\n","  if i == ['Y']:\n","    c[i] = c[i]\n","  else:\n","    c[i] = c[i].apply(lambda x: np.clip(x, a_min=np.quantile(x,q=0.02), a_max=np.quantile(x,q=0.98)))"],"metadata":{"id":"dFgw6cMquppn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d=df[1]\n","for i in a.columns:\n","  if i == ['Y']:\n","    d[i] = d[i]\n","  else:\n","    d[i] = d[i].apply(lambda x: np.clip(x, a_min=np.quantile(x,q=0.02), a_max=np.quantile(x,q=0.98)))"],"metadata":{"id":"mugk9oGxu1uf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clean_dataframes = [a, b, c ,d]"],"metadata":{"id":"uqsHXKcxv761"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test"],"metadata":{"id":"qqvNpDpzPpIa"}},{"cell_type":"code","source":["a_test =test_frames[0]\n","for i in a_test.columns:\n","  if i == ['Y']:\n","    a_test[i] = a_test[i]\n","  else:\n","    a_test[i] = a_test[i].apply(lambda x: np.clip(x, a_min=np.quantile(x,q=0.02), a_max=np.quantile(x,q=0.98)))"],"metadata":{"id":"EkKb6TAUPr4L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b_test =test_frames[1]\n","for i in b_test.columns:\n","  if i == ['Y']:\n","    b_test[i] = b_test[i]\n","  else:\n","    b_test[i] = b_test[i].apply(lambda x: np.clip(x, a_min=np.quantile(x,q=0.02), a_max=np.quantile(x,q=0.98)))"],"metadata":{"id":"YA62fb-vRGPl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c_test =test_frames[2]\n","for i in c_test.columns:\n","  if i == ['Y']:\n","    c_test[i] = c_test[i]\n","  else:\n","    c_test[i] = c_test[i].apply(lambda x: np.clip(x, a_min=np.quantile(x,q=0.02), a_max=np.quantile(x,q=0.98)))"],"metadata":{"id":"jcLlm6YkRUJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d_test =test_frames[3]\n","for i in d_test.columns:\n","  if i == ['Y']:\n","    d_test[i] = d_test[i]\n","  else:\n","    d_test[i] = d_test[i].apply(lambda x: np.clip(x, a_min=np.quantile(x,q=0.02), a_max=np.quantile(x,q=0.98)))"],"metadata":{"id":"PtnGebw3Rdlz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imp_feat_list = ['X5','X31','X51','X14','X59','X42','X53','X36','X6','X48'] #  ,'DAYS_EMPLOYED'\n","poly = [a[imp_feat_list], b[imp_feat_list], c[imp_feat_list], d[imp_feat_list]]\n","poly_test = [a_test[imp_feat_list], b_test[imp_feat_list], c_test[imp_feat_list], d_test[imp_feat_list]]"],"metadata":{"id":"mRTQvtRvlX6u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import PolynomialFeatures\n","poly_transform = PolynomialFeatures(degree=2)\n","\n","for i in range(4):\n","    poly[i] = poly_transform.fit_transform(poly[i])\n","    poly_feat_name_list = poly_transform.get_feature_names(imp_feat_list)\n","    poly[i] = pd.DataFrame(poly[i],columns=poly_feat_name_list)\n","    "],"metadata":{"id":"yDrm_b_Mla1-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" "],"metadata":{"id":"5mfD4bmfhYPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(4):\n","    poly[i].index = poly[i].index\n","    clean_dataframes[i] = poly[i].merge(clean_dataframes[i],how='inner', on = imp_feat_list)"],"metadata":{"id":"Qdr7IJkTljtr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dealing with imbalanced data"],"metadata":{"id":"drnAAffwY4nE"}},{"cell_type":"code","source":["def check_data_imbalance(dfs):\n","    for i in range(len(dfs)):\n","        print('Dataset: '+str(i+1)+'year')\n","        print(dfs[i].groupby('Y').size())\n","        try:\n","          minority_percent = (dfs[i]['Y'].tolist().count(1) / len(dfs[i]['Y'].tolist()))*100\n","        except:\n","          minority_percent = 0 \n","        print('Minority (label 1) percentage: '+  str(minority_percent) + '%')\n","        print('-'*64)\n","        \n","check_data_imbalance(clean_dataframes)"],"metadata":{"id":"5Pl7gx4PY_hM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_kfold_cv_data(k, X, y, verbose=False):\n","    X = X.values\n","    y = y.values\n","    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n","    X_train = []\n","    y_train = []\n","    X_test = []\n","    y_test = []\n","    \n","    for train_index, test_index in kf.split(X):\n","        X_train.append(X[train_index])\n","        y_train.append(y[train_index])\n","        X_test.append(X[test_index])\n","        y_test.append(y[test_index])\n","    return X_train, y_train, X_test, y_test"],"metadata":{"id":"H3mBbPe6M2tR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from xgboost import XGBClassifier\n"],"metadata":{"id":"HQqLmlo2VjG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["param_a = {'booster': 'gbtree',\n"," 'lambda': 1.433876126076925e-05,\n"," 'alpha': 5.541294137451031e-06,\n"," 'scale_pos_weight': 0.7194659025661956,\n"," 'max_depth': 9,\n"," 'eta': 0.7248402612848088,\n"," 'gamma': 0.0002853050773396981,\n"," 'grow_policy': 'depthwise'}\n","param_b = {'booster': 'dart',\n"," 'lambda': 2.7422411585568494e-06,\n"," 'alpha': 0.02194295318858507,\n"," 'scale_pos_weight': 0.9991694953298548,\n"," 'max_depth': 9,\n"," 'eta': 0.4081155559171879,\n"," 'gamma': 0.0002291703800060232,\n"," 'grow_policy': 'depthwise',\n"," 'sample_type': 'weighted',\n"," 'normalize_type': 'forest',\n"," 'rate_drop': 8.15129169703931e-07,\n"," 'skip_drop': 8.733696949171181e-07}\n","param_c = {'booster': 'gbtree',\n"," 'lambda': 6.9404480293999284e-06,\n"," 'alpha': 2.0844411765906503e-05,\n"," 'scale_pos_weight': 0.7518281741167833,\n"," 'max_depth': 7,\n"," 'eta': 0.4964415154222043,\n"," 'gamma': 6.187888926971437e-07,\n"," 'grow_policy': 'lossguide'}\n","param_d = {'booster': 'dart',\n"," 'lambda': 0.0004246095494636485,\n"," 'alpha': 0.0018844772828474623,\n"," 'scale_pos_weight': 0.9840603537740624,\n"," 'max_depth': 7,\n"," 'eta': 0.6859005312049079,\n"," 'gamma': 0.02316741557339404,\n"," 'grow_policy': 'lossguide',\n"," 'sample_type': 'weighted',\n"," 'normalize_type': 'tree',\n"," 'rate_drop': 4.589378989920688e-05,\n"," 'skip_drop': 2.653220112029294e-07}\n","param = [param_a, param_b, param_c, param_d]"],"metadata":{"id":"fR59T2Zsbq4E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clean_dataframes[1].head()"],"metadata":{"id":"EWnuhoNOuQ9k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_dataframes_features_labels(dfs):\n","    feature_dfs = [dfs[i].iloc[:,1:-2] for i in range(len(dfs))]\n","    label_dfs = [dfs[i].iloc[:,-1] for i in range(len(dfs))]\n","    return feature_dfs, label_dfs\n","\n","from sklearn.preprocessing import RobustScaler\n","scaler = RobustScaler()"],"metadata":{"id":"h2_a7M95tKV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pandas.core.common import random_state\n","feature_dfs, label_dfs = split_dataframes_features_labels(clean_dataframes)   \n","verbose= False\n","k_folds = 5\n","for df_index in range(len(clean_dataframes)):\n","  if verbose: print('\\t\\tDataset: ' + '\\033[1m' + str(df_index+1) + 'year' + '\\033[0m')\n","                \n","  # Calling the 'prepare_kfold_cv_data' returns lists of features and labels \n","  # for train and test sets respectively.\n","  # The number of items in the list is equal to k_folds\n","  X_train_list, y_train_list, X_test_list, y_test_list = prepare_kfold_cv_data(k_folds, feature_dfs[df_index], label_dfs[df_index], verbose)\n","\n","  #scaler.fit(feature_dfs[df_index])\n","  #X_train_list = scaler.fit_transform(X_train_list[df_index])\n","  #X_test_list = scaler.fit_transform(X_test_list[df_index])\n","  metrics_results = OrderedDict()\n","  accuracy_list = np.zeros([k_folds])\n","  precision_list = np.zeros([k_folds,2])\n","  recall_list = np.zeros([k_folds,2])\n","  TN_list = np.zeros([k_folds])\n","  FP_list = np.zeros([k_folds])\n","  FN_list = np.zeros([k_folds])\n","  TP_list = np.zeros([k_folds])                \n","  clf = XGBClassifier(**param[df_index])             \n","                # Iterate over all the k-folds\n","  for k_index in range(k_folds):\n","    X_train = X_train_list[k_index]\n","    y_train = y_train_list[k_index]\n","    X_test = X_test_list[k_index]\n","    y_test = y_test_list[k_index]\n","                    \n","    # Fit the model and \n","    \n","    clf.fit(X_train, y_train, verbose=True, early_stopping_rounds=1000, eval_metric='logloss', eval_set=[[X_test,y_test]])\n","    y_test_predicted = clf.predict(X_test)\n","                    \n","                    #code for calculating accuracy \n","    _accuracy_ = accuracy_score(y_test, y_test_predicted, normalize=True)\n","    accuracy_list[k_index] = _accuracy_\n","                    \n","                    #code for calculating recall \n","    _recalls_ = recall_score(y_test, y_test_predicted, average=None)\n","    recall_list[k_index] = _recalls_\n","                    \n","                    #code for calculating precision \n","    _precisions_ = precision_score(y_test, y_test_predicted, average=None)\n","    precision_list[k_index] = _precisions_\n","                    \n","                    #code for calculating confusion matrix \n","    _confusion_matrix_ = confusion_matrix(y_test, y_test_predicted)\n","    TN_list[k_index] = _confusion_matrix_[0][0]\n","    FP_list[k_index] = _confusion_matrix_[0][1]\n","    FN_list[k_index] = _confusion_matrix_[1][0]\n","    TP_list[k_index] = _confusion_matrix_[1][1]"],"metadata":{"id":"kjhk7RwwMZTg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["year_results= pd.DataFrame\n","metrics_results['Accuracy'] = np.mean(accuracy_list)\n","metrics_results['Precisions'] = np.mean(precision_list, axis=0)\n","metrics_results['Recalls'] = np.mean(recall_list, axis=0)\n","metrics_results['F1-score'] = (2*metrics_results['Precisions']*metrics_results['Recalls'])/(metrics_results['Precisions']+metrics_results['Recalls'])\n","metrics_results['TN'] = np.mean(TN_list)\n","metrics_results['FP'] = np.mean(FP_list)\n","metrics_results['FN'] = np.mean(FN_list)\n","metrics_results['TP'] = np.mean(TP_list)\n","                \n","if verbose:\n","  print('\\t\\t\\tAccuracy:', metrics_results['Accuracy'])\n","  print('\\t\\t\\tPrecision:', metrics_results['Precisions'])\n","  print('\\t\\t\\tRecall:', metrics_results['Recalls'])\n","                \n","metrics_results"],"metadata":{"id":"4smiz70BM3Fs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"tk61L3XNM3D_"},"execution_count":null,"outputs":[]}]}